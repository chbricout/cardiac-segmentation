{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"c:\\Users\\Brico\\miniconda3\\envs\\monai\\python.exe\"\n  * The NumPy version is: \"1.23.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: Le module spécifié est introuvable.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Brico\\miniconda3\\envs\\monai\\Lib\\site-packages\\numpy\\core\\__init__.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m multiarray\n\u001b[0;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Brico\\miniconda3\\envs\\monai\\Lib\\site-packages\\numpy\\core\\multiarray.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m overrides\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[1;32mc:\\Users\\Brico\\miniconda3\\envs\\monai\\Lib\\site-packages\\numpy\\core\\overrides.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_multiarray_umath\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inspect\u001b[39;00m \u001b[39mimport\u001b[39;00m getargspec\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: Le module spécifié est introuvable.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Brico\\OneDrive\\Code\\MTI865\\Challenge-H2023\\mainSegmentationChallenge.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Brico/OneDrive/Code/MTI865/Challenge-H2023/mainSegmentationChallenge.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Brico/OneDrive/Code/MTI865/Challenge-H2023/mainSegmentationChallenge.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Brico/OneDrive/Code/MTI865/Challenge-H2023/mainSegmentationChallenge.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mprogressBar\u001b[39;00m \u001b[39mimport\u001b[39;00m printProgressBar\n",
      "File \u001b[1;32mc:\\Users\\Brico\\miniconda3\\envs\\monai\\Lib\\site-packages\\torch\\__init__.py:1120\u001b[0m\n\u001b[0;32m   1115\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[39m# Define Storage and Tensor classes\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m-> 1120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m \u001b[39mimport\u001b[39;00m _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n\u001b[0;32m   1122\u001b[0m \u001b[39m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m \u001b[39m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mByteStorage\u001b[39;00m(_LegacyStorage):\n",
      "File \u001b[1;32mc:\\Users\\Brico\\miniconda3\\envs\\monai\\Lib\\site-packages\\torch\\storage.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     HAS_NUMPY \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Brico\\miniconda3\\envs\\monai\\Lib\\site-packages\\numpy\\__init__.py:140\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init\n\u001b[1;32m--> 140\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n",
      "File \u001b[1;32mc:\\Users\\Brico\\miniconda3\\envs\\monai\\Lib\\site-packages\\numpy\\core\\__init__.py:49\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39m \u001b[39m%\u001b[39m (sys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m], sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m], sys\u001b[39m.\u001b[39mexecutable,\n\u001b[0;32m     48\u001b[0m         __version__, exc)\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m     50\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m envkey \u001b[39min\u001b[39;00m env_added:\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"c:\\Users\\Brico\\miniconda3\\envs\\monai\\python.exe\"\n  * The NumPy version is: \"1.23.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: Le module spécifié est introuvable.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "from monai.networks.nets import UNet as MUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_to_seg(img, num_classes):\n",
    "    out = torch.zeros((1,*img.shape[1:])).to(img.device)\n",
    "    for i,layer in enumerate(img) :\n",
    "        out += layer*(i*255/(num_classes-1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import isnan\n",
    "\n",
    "\n",
    "def runTraining():\n",
    "    print('-' * 40)\n",
    "    print('~~~~~~~~  Starting the training... ~~~~~~')\n",
    "    print('-' * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    batch_size_val = 8\n",
    "    lr =   0.001  # Learning Rate\n",
    "    epoch = 50 # Number of epochs\n",
    "    \n",
    "    root_dir = './Data/'\n",
    "\n",
    "    print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform,\n",
    "                                                      mask_transform=mask_transform,\n",
    "                                                      augment=True,\n",
    "                                                      equalize=True)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                              batch_size=batch_size,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=batch_size_val,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = 'Test_Model'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    # net = UNet(num_classes)\n",
    "    net=MUNet(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "        channels=(4, 8, 16, 32, 64, 128),\n",
    "        strides=(1, 1, 1, 2, 2),\n",
    "    )\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax()\n",
    "    CE_loss = torch.nn.MSELoss( reduction=\"sum\")\n",
    "\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "    \n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "    \n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        DSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        \n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, img_names = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "            \n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = softMax(net(images))\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            encoded_classes = batch_one_hot_encode_2_chan(segmentation_classes, 2)\n",
    "            encoded_classes[:,1,:,:]=encoded_classes[:, -3:, :, :].sum(dim=1)\n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(net_predictions, encoded_classes[:,:2, :, :]) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            # element_by_layers = predToSegmentation(net_predictions).sum(dim=(2,3))\n",
    "            # prob_by_layer = (element_by_layers/(element_by_layers.sum()))\n",
    "            # piecewise_diff = prob_by_layer - torch.Tensor([0.8,0.2]).to(prob_by_layer.device)\n",
    "            # shannon = torch.pow(piecewise_diff, 2)\n",
    "            lossTotal = CE_loss_value\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            CE_loss_value.backward()\n",
    "            optimizer.step()\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            lossEpoch.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                             prefix=\"[Training] Epoch: {} \".format(i) ,\n",
    "                            length=15,\n",
    "                             suffix=\" Loss: {:.4f}, \".format(lossTotal))\n",
    "            labels.cpu()\n",
    "            images.cpu()\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                             done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch))\n",
    "\n",
    "        \n",
    "        ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH. \n",
    "        ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "        if not os.path.exists('./models/' + modelName):\n",
    "            os.makedirs('./models/' + modelName)\n",
    "        if i == epoch-1:\n",
    "            torch.save(net.state_dict(), './models/' + modelName + '/' + str(i) + '_Epoch')\n",
    "            \n",
    "        np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f81713-4c18-4dd3-bc72-35dd8f30a339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      " Dataset: ./Data/ \n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name: Test_Model\n",
      "Total params: 166,104\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "[Training] Epoch: 0 [DONE]                                      \n",
      "[Training] Epoch: 0, LossG: 547223.4375                                                                      \n",
      "[Training] Epoch: 1 [DONE]                                      \n",
      "[Training] Epoch: 1, LossG: 523626.6250                                                                      \n",
      "[Training] Epoch: 2 [DONE]                                      \n",
      "[Training] Epoch: 2, LossG: 518783.9688                                                                      \n",
      "[Training] Epoch: 3 [DONE]                                      \n",
      "[Training] Epoch: 3, LossG: 517136.9375                                                                      \n",
      "[Training] Epoch: 4 [DONE]                                      \n",
      "[Training] Epoch: 4, LossG: 516259.2812                                                                      \n",
      "[Training] Epoch: 5 [DONE]                                      \n",
      "[Training] Epoch: 5, LossG: 515764.0625                                                                      \n",
      "[Training] Epoch: 6 [DONE]                                      \n",
      "[Training] Epoch: 6, LossG: 515438.7812                                                                      \n",
      "[Training] Epoch: 7 [DONE]                                      \n",
      "[Training] Epoch: 7, LossG: 515140.3125                                                                      \n",
      "[Training] Epoch: 8 [DONE]                                      \n",
      "[Training] Epoch: 8, LossG: 514947.0000                                                                      \n",
      "[Training] Epoch: 9 [DONE]                                      \n",
      "[Training] Epoch: 9, LossG: 514822.4688                                                                      \n",
      "[Training] Epoch: 10 [DONE]                                      \n",
      "[Training] Epoch: 10, LossG: 514766.4375                                                                     \n",
      "[Training] Epoch: 11 [DONE]                                      \n",
      "[Training] Epoch: 11, LossG: 514654.1250                                                                     \n",
      "[Training] Epoch: 12 [DONE]                                      \n",
      "[Training] Epoch: 12, LossG: 514585.1562                                                                     \n",
      "[Training] Epoch: 13 [DONE]                                      \n",
      "[Training] Epoch: 13, LossG: 514545.0625                                                                     \n",
      "[Training] Epoch: 14 [DONE]                                      \n",
      "[Training] Epoch: 14, LossG: 514523.7812                                                                     \n",
      "[Training] Epoch: 15 [DONE]                                      \n",
      "[Training] Epoch: 15, LossG: 514422.4375                                                                     \n",
      "[Training] Epoch: 16 [DONE]                                      \n",
      "[Training] Epoch: 16, LossG: 514423.9688                                                                     \n",
      "[Training] Epoch: 17 [DONE]                                      \n",
      "[Training] Epoch: 17, LossG: 514338.7812                                                                     \n",
      "[Training] Epoch: 18 [DONE]                                      \n",
      "[Training] Epoch: 18, LossG: 514328.1875                                                                     \n",
      "[Training] Epoch: 19 [DONE]                                      \n",
      "[Training] Epoch: 19, LossG: 514343.2812                                                                     \n",
      "[Training] Epoch: 20 [DONE]                                      \n",
      "[Training] Epoch: 20, LossG: 514338.3438                                                                     \n",
      "[Training] Epoch: 21 [DONE]                                      \n",
      "[Training] Epoch: 21, LossG: 514311.6562                                                                     \n",
      "[Training] Epoch: 22 [DONE]                                      \n",
      "[Training] Epoch: 22, LossG: 514252.9375                                                                     \n",
      "[Training] Epoch: 23 [DONE]                                      \n",
      "[Training] Epoch: 23, LossG: 514147.7188                                                                     \n",
      "[Training] Epoch: 24 [DONE]                                      \n",
      "[Training] Epoch: 24, LossG: 514195.8125                                                                     \n",
      "[Training] Epoch: 25 [DONE]                                      \n",
      "[Training] Epoch: 25, LossG: 514143.1875                                                                     \n",
      "[Training] Epoch: 26 [DONE]                                      \n",
      "[Training] Epoch: 26, LossG: 514108.9688                                                                     \n",
      "[Training] Epoch: 27 [DONE]                                      \n",
      "[Training] Epoch: 27, LossG: 514067.4375                                                                     \n",
      "[Training] Epoch: 28 [DONE]                                      \n",
      "[Training] Epoch: 28, LossG: 514044.2188                                                                     \n",
      "[Training] Epoch: 29 [DONE]                                      \n",
      "[Training] Epoch: 29, LossG: 514059.8438                                                                     \n",
      "[Training] Epoch: 30 [DONE]                                      \n",
      "[Training] Epoch: 30, LossG: 514149.2188                                                                     \n",
      "[Training] Epoch: 31 [DONE]                                      \n",
      "[Training] Epoch: 31, LossG: 514093.4688                                                                     \n",
      "[Training] Epoch: 32 [DONE]                                      \n",
      "[Training] Epoch: 32, LossG: 514104.7188                                                                     \n",
      "[Training] Epoch: 33 [DONE]                                      \n",
      "[Training] Epoch: 33, LossG: 514044.2188                                                                     \n",
      "[Training] Epoch: 34 [DONE]                                      \n",
      "[Training] Epoch: 34, LossG: 514037.6875                                                                     \n",
      "[Training] Epoch: 35 [DONE]                                      \n",
      "[Training] Epoch: 35, LossG: 514095.5625                                                                     \n",
      "[Training] Epoch: 36 [DONE]                                      \n",
      "[Training] Epoch: 36, LossG: 514141.6875                                                                     \n",
      "[Training] Epoch: 37 [DONE]                                      \n",
      "[Training] Epoch: 37, LossG: 513999.9375                                                                     \n",
      "[Training] Epoch: 38 [DONE]                                      \n",
      "[Training] Epoch: 38, LossG: 514044.5625                                                                     \n",
      "[Training] Epoch: 39 [DONE]                                      \n",
      "[Training] Epoch: 39, LossG: 513994.9375                                                                     \n",
      "[Training] Epoch: 40 [DONE]                                      \n",
      "[Training] Epoch: 40, LossG: 513970.7812                                                                     \n",
      "[Training] Epoch: 41 [DONE]                                      \n",
      "[Training] Epoch: 41, LossG: 513993.7812                                                                     \n",
      "[Training] Epoch: 42 [DONE]                                      \n",
      "[Training] Epoch: 42, LossG: 513952.0625                                                                     \n",
      "[Training] Epoch: 43 [DONE]                                      \n",
      "[Training] Epoch: 43, LossG: 513918.8125                                                                     \n",
      "[Training] Epoch: 44 [DONE]                                      \n",
      "[Training] Epoch: 44, LossG: 513986.6250                                                                     \n",
      "[Training] Epoch: 45 [DONE]                                      \n",
      "[Training] Epoch: 45, LossG: 513941.3438                                                                     \n",
      "[Training] Epoch: 46 [DONE]                                      \n",
      "[Training] Epoch: 46, LossG: 513956.6250                                                                     \n",
      "[Training] Epoch: 47 [DONE]                                      \n",
      "[Training] Epoch: 47, LossG: 513867.9688                                                                     \n",
      "[Training] Epoch: 48 [DONE]                                      \n",
      "[Training] Epoch: 48, LossG: 513817.4375                                                                     \n",
      "[Training] Epoch: 49 [DONE]                                      \n",
      "[Training] Epoch: 49, LossG: 513809.5625                                                                     \n"
     ]
    }
   ],
   "source": [
    "testnet = runTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ae10184-bacf-4c4d-9767-3272a76a0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = UNet(num_classes=4).cuda()\n",
    "# mod.load_state_dict(torch.load(\"models/Test_Model/49_Epoch\"))\n",
    "# mod=testnet\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "val_set = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    \"./Data/\",\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "val_loader = DataLoader(val_set,\n",
    "                            batch_size=2,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "# inference(mod, val_loader, \"Test_Model\", 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask, path = val_set[10]\n",
    "out = mod(to_var(img)[None,:])\n",
    "out_cpu =out.detach().cpu()[0]\n",
    "out_cpu = torch.nn.functional.softmax(out_cpu, dim=0)\n",
    "segmentation_image = out_to_seg(predToSegmentation(out_cpu),3)\n",
    "\n",
    "# Now, concatenated_output has shape (batch_size, 1, num_classes, height, width)\n",
    "# If you want to remove the singleton dimension, you can use squeeze\n",
    "print(out_cpu.max(dim=0))\n",
    "fig, plots = plt.subplots(1,2, figsize=(20,20))\n",
    "plots[0].imshow(segmentation_image.permute(1,2,0), cmap=\"gray\")\n",
    "plots[1].imshow(mask.permute(1,2,0), cmap=\"gray\")\n",
    "mask.unique(), path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Brico\\OneDrive\\Code\\MTI865\\Challenge-H2023\\mainSegmentationChallenge.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Brico/OneDrive/Code/MTI865/Challenge-H2023/mainSegmentationChallenge.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m val_set[:\u001b[39m10\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\Brico\\OneDrive\\Code\\MTI865\\Challenge-H2023\\medicalDataLoader.py:108\u001b[0m, in \u001b[0;36mMedicalImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 108\u001b[0m     img_path, mask_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs[index]\n\u001b[0;32m    109\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[0;32m    110\u001b[0m     mask \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(mask_path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "val_set[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
